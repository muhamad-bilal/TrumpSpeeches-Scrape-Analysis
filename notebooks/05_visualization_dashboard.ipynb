{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trump Speech Analysis - Visualization Dashboard\n",
        "\n",
        "Comprehensive visualizations using Plotly (interactive) and Matplotlib/Seaborn (publication-ready).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from pathlib import Path\n",
        "from wordcloud import WordCloud\n",
        "import json\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load feature data\n",
        "data_dir = Path('../data/transformed')\n",
        "csv_files = list(data_dir.glob('speeches_features_complete_*.csv'))\n",
        "\n",
        "if csv_files:\n",
        "    latest_file = max(csv_files, key=lambda p: p.stat().st_mtime)\n",
        "    df = pd.read_csv(latest_file)\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    print(f\"Loaded {len(df)} speeches with {len(df.columns)} features\")\n",
        "else:\n",
        "    print(\"No data found\")\n",
        "\n",
        "# Load transformed data for text analysis\n",
        "json_files = list(data_dir.glob('speeches_nlp_features_*.json'))\n",
        "if json_files:\n",
        "    latest_json = max(json_files, key=lambda p: p.stat().st_mtime)\n",
        "    with open(latest_json, 'r', encoding='utf-8') as f:\n",
        "        transformed_data = json.load(f)\n",
        "    print(f\"Loaded {len(transformed_data)} transformed speeches\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Interactive Sentiment Timeline (Plotly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'sentiment_compound' in df.columns and 'date' in df.columns:\n",
        "    df_sorted = df.sort_values('date').dropna(subset=['date'])\n",
        "    \n",
        "    fig = px.line(df_sorted, x='date', y='sentiment_compound', \n",
        "                  title='Sentiment Timeline Across Trump Speeches',\n",
        "                  labels={'date': 'Date', 'sentiment_compound': 'Sentiment (Compound)'},\n",
        "                  hover_data=['title'])\n",
        "    \n",
        "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", opacity=0.5)\n",
        "    fig.update_layout(height=500)\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Word Cloud from All Speeches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate word cloud from cleaned text\n",
        "cleaned_dir = Path('../data/cleaned')\n",
        "csv_files = list(cleaned_dir.glob('speeches_cleaned_*.csv'))\n",
        "\n",
        "if csv_files:\n",
        "    latest_cleaned = max(csv_files, key=lambda p: p.stat().st_mtime)\n",
        "    df_text = pd.read_csv(latest_cleaned)\n",
        "    \n",
        "    all_text = ' '.join(df_text['cleaned_text'].dropna())\n",
        "    \n",
        "    wordcloud = WordCloud(width=1200, height=600, background_color='white',\n",
        "                          colormap='viridis', max_words=100).generate(all_text)\n",
        "    \n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud - All Trump Speeches', fontsize=20, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Emotion Heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Emotion heatmap\n",
        "emotion_cols = [c for c in df.columns if c.startswith('emotion_')]\n",
        "\n",
        "if emotion_cols and 'title' in df.columns:\n",
        "    emotion_data = df[['title'] + emotion_cols].set_index('title')\n",
        "    emotion_data.columns = [c.replace('emotion_', '').title() for c in emotion_data.columns]\n",
        "    \n",
        "    plt.figure(figsize=(14, 10))\n",
        "    sns.heatmap(emotion_data, cmap='RdYlGn', center=0, cbar_kws={'label': 'Emotion Score'})\n",
        "    plt.title('Emotion Heatmap Across Speeches', fontsize=16, pad=20)\n",
        "    plt.xlabel('Emotion')\n",
        "    plt.ylabel('Speech')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. N-gram Frequency Bar Charts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract top n-grams from transformed data\n",
        "if 'transformed_data' in locals():\n",
        "    from collections import Counter\n",
        "    \n",
        "    all_bigrams = Counter()\n",
        "    all_trigrams = Counter()\n",
        "    \n",
        "    for speech in transformed_data:\n",
        "        ngrams_data = speech.get('ngrams', {})\n",
        "        if '2gram' in ngrams_data:\n",
        "            for gram, count in ngrams_data['2gram']:\n",
        "                all_bigrams[gram] += count\n",
        "        if '3gram' in ngrams_data:\n",
        "            for gram, count in ngrams_data['3gram']:\n",
        "                all_trigrams[gram] += count\n",
        "    \n",
        "    # Plot top bigrams\n",
        "    if all_bigrams:\n",
        "        top_bigrams = dict(all_bigrams.most_common(15))\n",
        "        \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.barh(list(top_bigrams.keys()), list(top_bigrams.values()), color='steelblue')\n",
        "        plt.xlabel('Frequency')\n",
        "        plt.title('Top 15 Bigrams Across All Speeches', fontsize=14)\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Readability Trends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Readability over time\n",
        "if 'readability_flesch_kincaid_grade' in df.columns and 'date' in df.columns:\n",
        "    df_sorted = df.sort_values('date').dropna(subset=['date'])\n",
        "    \n",
        "    fig = make_subplots(rows=2, cols=1, \n",
        "                        subplot_titles=('Flesch-Kincaid Grade Level', 'Flesch Reading Ease'))\n",
        "    \n",
        "    fig.add_trace(go.Scatter(x=df_sorted['date'], \n",
        "                             y=df_sorted['readability_flesch_kincaid_grade'],\n",
        "                             mode='lines+markers', name='FK Grade'),\n",
        "                  row=1, col=1)\n",
        "    \n",
        "    if 'readability_flesch_reading_ease' in df.columns:\n",
        "        fig.add_trace(go.Scatter(x=df_sorted['date'], \n",
        "                                 y=df_sorted['readability_flesch_reading_ease'],\n",
        "                                 mode='lines+markers', name='Reading Ease',\n",
        "                                 line=dict(color='orange')),\n",
        "                      row=2, col=1)\n",
        "    \n",
        "    fig.update_xaxes(title_text=\"Date\")\n",
        "    fig.update_yaxes(title_text=\"Score\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Score\", row=2, col=1)\n",
        "    fig.update_layout(height=700, title_text=\"Readability Trends Over Time\")\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Political Theme Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Political keyword clusters\n",
        "keyword_cols = [c for c in df.columns if c.startswith('keywords_') and c != 'keywords_total']\n",
        "\n",
        "if keyword_cols:\n",
        "    keyword_totals = df[keyword_cols].sum()\n",
        "    keyword_totals.index = [c.replace('keywords_', '').replace('_', ' ').title() for c in keyword_totals.index]\n",
        "    \n",
        "    # Pie chart with Plotly\n",
        "    fig = px.pie(values=keyword_totals.values, names=keyword_totals.index,\n",
        "                 title='Distribution of Political Themes')\n",
        "    fig.update_traces(textposition='inside', textinfo='percent+label')\n",
        "    fig.update_layout(height=600)\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Psychological Features - Pronoun Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pronoun usage patterns\n",
        "pronoun_cols = [c for c in df.columns if c.startswith('pronoun_')]\n",
        "\n",
        "if pronoun_cols:\n",
        "    pronoun_means = df[pronoun_cols].mean()\n",
        "    pronoun_means.index = [c.replace('pronoun_', '').replace('_', ' ').title() for c in pronoun_means.index]\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    pronoun_means.sort_values(ascending=True).plot(kind='barh', color='coral')\n",
        "    plt.xlabel('Average Count per Speech')\n",
        "    plt.title('Pronoun Usage Patterns', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
