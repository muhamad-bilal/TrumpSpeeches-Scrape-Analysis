\documentclass[12pt, a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage{algorithm}
\usepackage{algpseudocode}

% ============================================================================
% PAGE SETUP
% ============================================================================
\geometry{
    a4paper,
    left=25mm,
    right=25mm,
    top=25mm,
    bottom=25mm,
    headheight=15pt
}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Trump Speech Analysis}
\fancyhead[R]{Data Analytics Report}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% Code listing style
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=5pt,
    frame=single,
    framesep=5pt,
    tabsize=4,
    language=Python
}
\lstset{style=pythonstyle}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green
}

% ============================================================================
% TITLE PAGE
% ============================================================================
\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Huge\bfseries Comprehensive Data Analysis of\\Donald Trump's Speech Transcripts\par}
    
    \vspace{1cm}
    
    {\Large A Complete NLP Pipeline with Predictive Analytics\par}
    
    \vspace{2cm}
    
    {\large\textbf{Course:} Database Analytics (DBA)\\
    \textbf{Semester:} 7th Semester\par}
    
    \vspace{1.5cm}
    
    {\large\textbf{Submitted by:}\\
    Muhammad Bilal\par}
    
    \vspace{2cm}
    
    {\large December 2025\par}
    
\end{titlepage}

% ============================================================================
% ABSTRACT
% ============================================================================
\newpage
\begin{abstract}
This report presents the predictive analytics and behavioral modeling phase of the Trump Speech Analysis project. Building upon the previously submitted data warehousing documentation, this report focuses on the NLP transformation pipeline, feature engineering methodology, descriptive analytics findings, and six predictive models for behavioral analysis. The pipeline processes 43 speech transcripts containing over 320,000 words, with 2,872 unique entities extracted and 80+ engineered features computed. The key contribution is the development of the ``Mega Trump Model'' --- a unified predictive system combining entity reaction profiling, personality compatibility prediction, negotiation success modeling, ML-based response classification, psychological influence strategies, and trigger word detection.
\end{abstract}

% ============================================================================
% TABLE OF CONTENTS
% ============================================================================
\newpage
\tableofcontents
\newpage

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}

\subsection{Project Overview}
This project implements a complete end-to-end data analytics pipeline for analyzing Donald Trump's speech transcripts. The pipeline encompasses:

\begin{enumerate}
    \item \textbf{Data Collection:} Web scraping from Rev.com transcription service
    \item \textbf{Data Cleaning:} Text preprocessing and normalization
    \item \textbf{Data Transformation:} Advanced NLP processing
    \item \textbf{Feature Engineering:} Deriving analytical features
    \item \textbf{Data Warehousing:} Star schema design with ETL pipeline
    \item \textbf{Descriptive Analytics:} Statistical analysis and visualization
    \item \textbf{Predictive Analytics:} Machine learning and rule-based models
\end{enumerate}

\subsection{Objectives}
The primary objectives of this analysis are:
\begin{itemize}
    \item Extract and clean speech transcripts from online sources
    \item Apply Natural Language Processing (NLP) techniques to extract linguistic features
    \item Perform sentiment analysis and emotion classification
    \item Extract named entities (persons, organizations, locations)
    \item Design and implement a data warehouse using star schema
    \item Conduct comprehensive descriptive analytics
    \item Build predictive models for behavioral analysis
    \item Create an interactive dashboard for model exploration
\end{itemize}

\subsection{Scope of This Report}
This report focuses primarily on the \textbf{new contributions} beyond the previously submitted data warehousing documentation:
\begin{itemize}
    \item Detailed NLP transformation methodology
    \item Feature engineering approaches (80+ derived attributes)
    \item Descriptive analytics results and findings
    \item \textbf{Six predictive analytics models} (new)
    \item \textbf{Mega Trump Model} -- unified behavioral predictor (new)
    \item Interactive Streamlit dashboard (new)
\end{itemize}

\subsection{Dataset Summary}
\begin{table}[H]
    \centering
    \caption{Dataset Statistics}
    \begin{tabular}{ll}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Total Speeches & 43 \\
        Total Words & 320,000+ \\
        Average Words per Speech & 7,400+ \\
        Unique Entities Extracted & 2,872 \\
        Engineered Features & 80+ \\
        Time Period & 2024-2025 \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Note:} The data warehousing aspects (ETL pipeline, star schema design, entity extraction methodology) were documented in the previously submitted report. This report focuses on the NLP transformation, descriptive analytics, and predictive modeling components.

% ============================================================================
% 2. DATA COLLECTION
% ============================================================================
\section{Data Collection: Web Scraping}

\subsection{Data Source}
The speech transcripts were collected from \textbf{Rev.com}, a professional transcription service that provides accurate transcriptions of political speeches, press conferences, and public addresses.

\subsection{Scraping Implementation}
The scraping module was implemented using:
\begin{itemize}
    \item \textbf{Selenium WebDriver:} For dynamic content rendering
    \item \textbf{BeautifulSoup:} For HTML parsing
    \item \textbf{Chrome (Headless):} Browser automation
\end{itemize}

\subsubsection{Key Features of the Scraper}
\begin{lstlisting}[caption={TrumpTranscriptScraper Class Structure}]
class TrumpTranscriptScraper:
    def __init__(self, driver_path=None):
        self.base_url = "https://www.rev.com"
        self.driver = None
        self.speeches = []
        self.known_urls = [...]  # 44 pre-identified URLs
    
    def setup_driver(self):
        # Headless Chrome configuration
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        
    def scrape_transcript(self, url):
        # Extract: title, date, transcript text
        # Multiple parsing strategies for robustness
        
    def save_results(self):
        # Save to JSON and CSV formats
\end{lstlisting}

\subsection{Data Extraction Strategy}
The scraper employs multiple parsing strategies to ensure robust extraction:

\begin{enumerate}
    \item \textbf{Primary Strategy:} Extract from \texttt{main-content} div
    \item \textbf{Secondary Strategy:} Extract from \texttt{fs-toc-element='contents'} attribute
    \item \textbf{Tertiary Strategy:} Extract from article body containers
    \item \textbf{Fallback Strategy:} Filter all paragraphs by length and content
\end{enumerate}

\subsection{Output Format}
Each scraped transcript contains:
\begin{table}[H]
    \centering
    \caption{Raw Data Schema}
    \begin{tabular}{lll}
        \toprule
        \textbf{Field} & \textbf{Type} & \textbf{Description} \\
        \midrule
        url & string & Source URL \\
        title & string & Speech title \\
        date & string & Date of speech \\
        transcript & string & Full transcript text \\
        scraped\_at & datetime & Scraping timestamp \\
        \bottomrule
    \end{tabular}
\end{table}

% ============================================================================
% 3. DATA CLEANING
% ============================================================================
\section{Data Cleaning Pipeline}

\subsection{Cleaning Operations}
The data cleaning pipeline performs comprehensive text preprocessing:

\subsubsection{HTML Tag Removal}
All residual HTML markup is stripped using regex patterns and BeautifulSoup parsing.

\subsubsection{Timestamp Removal}
Transcript timestamps in various formats are removed:
\begin{lstlisting}
# Patterns removed:
[HH:MM:SS], [MM:SS]
(HH:MM:SS), (MM:SS)
\end{lstlisting}

\subsubsection{Reaction Tag Removal}
Audience reactions and editorial annotations are removed:
\begin{itemize}
    \item (Applause), [Applause]
    \item (Cheers), [Cheers]
    \item (Laughter), [Laughter]
    \item (Inaudible), [Crosstalk]
\end{itemize}

\subsubsection{Speaker Standardization}
Speaker tags are normalized to consistent format:
\begin{lstlisting}
# Before: "Donald Trump:", "President Trump:", "Trump:"
# After: "TRUMP:"
\end{lstlisting}

\subsubsection{Noise Token Removal}
Common noise tokens and metadata are filtered:
\begin{itemize}
    \item ``Transcript''
    \item ``Rev.com''
    \item Copyright notices
    \item Fair use disclaimers
\end{itemize}

\subsubsection{Punctuation Normalization}
\begin{itemize}
    \item Smart quotes converted to standard quotes
    \item Multiple consecutive punctuation marks reduced
    \item Spacing after sentence terminators ensured
\end{itemize}

\subsubsection{Duplicate Removal}
Consecutive duplicate paragraphs are identified and removed to eliminate scraping artifacts.

\subsection{Cleaning Statistics}
\begin{table}[H]
    \centering
    \caption{Data Cleaning Results}
    \begin{tabular}{ll}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Speeches Processed & 44 \\
        Average Character Reduction & 15-20\% \\
        UTF-8 Validation & 100\% Pass \\
        Duplicate Paragraphs Removed & ~200 \\
        \bottomrule
    \end{tabular}
\end{table}

% ============================================================================
% 4. DATA TRANSFORMATION
% ============================================================================
\section{Data Transformation: NLP Pipeline}

\subsection{NLP Tools and Libraries}
The transformation pipeline utilizes:
\begin{itemize}
    \item \textbf{spaCy} (en\_core\_web\_sm): Tokenization, POS tagging, NER
    \item \textbf{NLTK:} N-gram extraction, stopwords
    \item \textbf{VADER:} Sentiment analysis
    \item \textbf{NRCLex:} Emotion classification
    \item \textbf{textstat:} Readability metrics
    \item \textbf{Sentence-Transformers:} Semantic embeddings
\end{itemize}

\subsection{Transformation Operations}

\subsubsection{Sentence Segmentation}
Text is segmented into sentences using spaCy's sentence boundary detection:
\begin{lstlisting}
def segment_sentences(self, text):
    doc = self.nlp(text)
    return [sent.text.strip() for sent in doc.sents]
\end{lstlisting}

\subsubsection{Tokenization and POS Tagging}
Each text is tokenized and annotated with:
\begin{itemize}
    \item Tokens (words)
    \item Part-of-speech (POS) tags
    \item Lemmas (base forms)
\end{itemize}

\subsubsection{Named Entity Recognition (NER)}
Eight entity types are extracted:
\begin{table}[H]
    \centering
    \caption{Named Entity Types}
    \begin{tabular}{ll}
        \toprule
        \textbf{Type} & \textbf{Description} \\
        \midrule
        PERSON & People, including fictional characters \\
        ORG & Companies, agencies, institutions \\
        GPE & Countries, cities, states \\
        DATE & Absolute or relative dates \\
        MONEY & Monetary values \\
        NORP & Nationalities, religious/political groups \\
        FAC & Buildings, airports, highways \\
        LOC & Non-GPE locations (mountains, rivers) \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Sentiment Analysis (VADER)}
VADER (Valence Aware Dictionary and sEntiment Reasoner) provides:
\begin{itemize}
    \item \textbf{Positive Score:} Proportion of positive sentiment (0-1)
    \item \textbf{Negative Score:} Proportion of negative sentiment (0-1)
    \item \textbf{Neutral Score:} Proportion of neutral sentiment (0-1)
    \item \textbf{Compound Score:} Normalized weighted composite (-1 to +1)
\end{itemize}

The compound score formula:
\begin{equation}
    \text{compound} = \frac{\sum_i v_i}{\sqrt{(\sum_i v_i)^2 + \alpha}}
\end{equation}
where $v_i$ are valence scores and $\alpha$ is a normalization constant.

\subsubsection{Emotion Classification}
Using NRCLex lexicon, eight emotions are scored:
\begin{itemize}
    \item Anger, Fear, Joy, Sadness
    \item Surprise, Disgust, Trust, Anticipation
\end{itemize}

\subsubsection{Readability Metrics}
Multiple readability formulas are computed:

\textbf{Flesch-Kincaid Grade Level:}
\begin{equation}
    \text{FK} = 0.39 \cdot \frac{\text{words}}{\text{sentences}} + 11.8 \cdot \frac{\text{syllables}}{\text{words}} - 15.59
\end{equation}

\textbf{Flesch Reading Ease:}
\begin{equation}
    \text{FRE} = 206.835 - 1.015 \cdot \frac{\text{words}}{\text{sentences}} - 84.6 \cdot \frac{\text{syllables}}{\text{words}}
\end{equation}

\textbf{Gunning Fog Index:}
\begin{equation}
    \text{GF} = 0.4 \cdot \left( \frac{\text{words}}{\text{sentences}} + 100 \cdot \frac{\text{complex words}}{\text{words}} \right)
\end{equation}

\subsubsection{N-gram Extraction}
Unigrams, bigrams, and trigrams are extracted with frequency counts, excluding stopwords.

\subsubsection{Semantic Embeddings}
384-dimensional sentence embeddings are generated using \texttt{all-MiniLM-L6-v2} model for semantic similarity analysis.

% ============================================================================
% 5. FEATURE ENGINEERING
% ============================================================================
\section{Feature Engineering}

\subsection{Feature Categories}
Over 80 features are engineered across six categories:

\subsubsection{Linguistic Features}
\begin{itemize}
    \item Average sentence length
    \item Standard deviation of sentence length
    \item Type-token ratio (lexical diversity)
    \item Average word length
    \item Unique word count
    \item All readability metrics
\end{itemize}

\textbf{Type-Token Ratio (TTR):}
\begin{equation}
    \text{TTR} = \frac{|\text{unique tokens}|}{|\text{total tokens}|}
\end{equation}

\subsubsection{Rhetorical Features}
\begin{itemize}
    \item Anaphora patterns (``We will...'', ``I am...'', ``They are...'')
    \item Contrast markers (``but'', ``however'', ``although'')
    \item Repetition density
    \item Alliteration count
    \item Superlative usage (``best'', ``greatest'', ``most'')
\end{itemize}

\subsubsection{Political/Thematic Features}
Keyword clusters for major themes:
\begin{itemize}
    \item Economy: ``jobs'', ``trade'', ``economy'', ``taxes'', ``business''
    \item Security: ``military'', ``defense'', ``border'', ``terrorism''
    \item Immigration: ``immigration'', ``border'', ``wall'', ``visa''
    \item Foreign Policy: ``China'', ``Russia'', ``NATO'', ``allies''
\end{itemize}

\subsubsection{Emotional Features}
\begin{itemize}
    \item Overall sentiment (pos, neg, neu, compound)
    \item Sentiment statistics (mean, variance, range)
    \item Eight emotion scores
    \item Dominant emotion identification
    \item Emotional volatility (sentiment standard deviation)
\end{itemize}

\subsubsection{Psychological Features}
\begin{itemize}
    \item \textbf{Pronoun Analysis:}
        \begin{itemize}
            \item First person singular (``I'', ``me'', ``my'')
            \item First person plural (``we'', ``us'', ``our'')
            \item Second person (``you'', ``your'')
            \item I/We ratio (ego vs. collective focus)
        \end{itemize}
    \item \textbf{Modal Verb Usage:} ``will'', ``should'', ``must'', ``can''
    \item \textbf{Certainty Markers:} ``absolutely'', ``definitely'', ``clearly''
    \item \textbf{Power vs. Affiliation Words:}
        \begin{itemize}
            \item Power: ``strong'', ``control'', ``dominate''
            \item Affiliation: ``together'', ``team'', ``support''
        \end{itemize}
\end{itemize}

\subsubsection{Stylistic Features}
\begin{itemize}
    \item Adjective/Adverb ratio
    \item Question count
    \item Exclamation count
    \item All-caps word count (emphasis)
\end{itemize}

% ============================================================================
% 6. DATA WAREHOUSING (Summary)
% ============================================================================
\section{Data Warehousing Summary}

\textbf{Note:} The complete data warehousing documentation, including detailed star schema design, ETL pipeline architecture, entity extraction methodology, and database schema definitions, was submitted in the previous report titled ``Trump Speech Analysis Data Warehouse Documentation.''

This section provides a brief summary of the key components for context:

\subsection{Schema Overview}
The data warehouse implements a \textbf{star schema} with:
\begin{itemize}
    \item \textbf{1 Fact Table:} \texttt{fact\_speech\_metrics} containing 40+ computed metrics per speech
    \item \textbf{6 Dimension Tables:} Speech, Person, Organization, Location, Date, Topic
    \item \textbf{4 Bridge Tables:} For many-to-many relationships (speech-entity mappings)
\end{itemize}

\subsection{Key Metrics Stored}
The fact table stores metrics computed during feature engineering:
\begin{itemize}
    \item Sentiment scores (VADER compound, positive, negative, neutral)
    \item Eight emotion scores (NRCLex)
    \item Readability metrics (Flesch-Kincaid, Gunning Fog, SMOG)
    \item Linguistic features (lexical diversity, sentence length statistics)
    \item Psychological indicators (pronoun ratios, power/affiliation words)
    \item Entity counts by type
\end{itemize}

\subsection{ETL Pipeline}
The three-stage ETL pipeline extracts data from the NLP outputs, preprocesses with normalization and surrogate key generation, and loads into the warehouse schema. See previous report for implementation details.

% ============================================================================
% 7. DESCRIPTIVE ANALYTICS
% ============================================================================
\section{Descriptive Analytics}

\subsection{Linguistic Analysis}

\subsubsection{Readability Statistics}
\begin{table}[H]
    \centering
    \caption{Readability Metrics Summary (Approximate)}
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Metric} & \textbf{Mean} & \textbf{Min} & \textbf{Max} \\
        \midrule
        Flesch Reading Ease & 75-77 & 65 & 85 \\
        Flesch-Kincaid Grade & 5.5-6.0 & 4.0 & 8.0 \\
        Gunning Fog Index & 7.5-8.5 & 6.0 & 10.0 \\
        \bottomrule
    \end{tabular}
\end{table}

The Flesch-Kincaid grade level of approximately 5-6 indicates speeches are highly accessible, targeting a 5th-6th grade reading level. This is consistent with political communication strategies designed to reach broad audiences.

\subsubsection{Lexical Diversity}
\begin{itemize}
    \item Mean Type-Token Ratio: 0.13-0.15
    \item Lower TTR reflects repetitive rhetorical style common in persuasive speeches
    \item Longer speeches naturally have lower TTR due to word reuse
\end{itemize}

\subsection{Rhetorical Analysis}

\subsubsection{Anaphora Patterns}
Common anaphoric patterns detected:
\begin{itemize}
    \item ``We will...'' (most frequent)
    \item ``I am...''
    \item ``They are...''
    \item ``We are...''
\end{itemize}

\subsubsection{Superlative Usage}
High frequency of superlatives (``best'', ``greatest'', ``most'', ``biggest'') characteristic of Trump's rhetorical style.

\subsection{Emotional Analysis}

\subsubsection{Sentiment Distribution}
\begin{table}[H]
    \centering
    \caption{Sentiment Classification (VADER Compound Score)}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Category} & \textbf{Count} & \textbf{Percentage} \\
        \midrule
        Positive (compound > 0.05) & 42 & 97.7\% \\
        Neutral (-0.05 to 0.05) & 1 & 2.3\% \\
        Negative (compound < -0.05) & 0 & 0.0\% \\
        \bottomrule
    \end{tabular}
\end{table}

The overwhelmingly positive sentiment (compound scores approaching 1.0) reflects the promotional and optimistic framing typical of political rally speeches. Individual sentence-level sentiment shows more variance (ranging from -0.95 to +0.95), but aggregated speech-level sentiment is consistently positive.

\subsubsection{Emotion Distribution}
Average emotion scores reveal:
\begin{itemize}
    \item \textbf{Trust} and \textbf{Anticipation:} Highest scores
    \item \textbf{Joy:} Moderate presence
    \item \textbf{Fear} and \textbf{Anger:} Present but controlled
    \item \textbf{Sadness} and \textbf{Disgust:} Lowest scores
\end{itemize}

\subsection{Psychological Profiling}

\subsubsection{Pronoun Analysis}
\begin{itemize}
    \item Mean I/We ratio: > 1.0 (ego-focused)
    \item High first-person singular usage indicates personal attribution style
    \item ``We'' usage increases in policy discussions
\end{itemize}

\subsubsection{Power vs. Affiliation}
\begin{itemize}
    \item Power/Affiliation ratio: > 1.0 (power-oriented)
    \item Consistent with authoritative communication style
\end{itemize}

\subsection{Named Entity Analysis}

\subsubsection{Entity Distribution by Type}
\begin{table}[H]
    \centering
    \caption{Entity Type Distribution (Total: 2,872 unique entities)}
    \begin{tabular}{lr}
        \toprule
        \textbf{Entity Type} & \textbf{Unique Count} \\
        \midrule
        DATE & 915 \\
        PERSON & 696 \\
        ORG & 491 \\
        GPE & 287 \\
        MONEY & 242 \\
        NORP & 108 \\
        LOC & 87 \\
        FAC & 46 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Most Mentioned Entities}
Top entities by mention frequency include political figures, countries (China, Russia, Ukraine), and organizations (NATO, FBI, DOJ).

% ============================================================================
% 8. PREDICTIVE ANALYTICS
% ============================================================================
\section{Predictive Analytics}

\subsection{Overview}
Six predictive models were developed to analyze Trump's behavioral patterns:

\begin{enumerate}
    \item Entity Reaction Profiler
    \item Personality Compatibility Predictor
    \item Negotiation Success Predictor
    \item Response Classifier (ML)
    \item Psychological Influence Model
    \item Trigger Word Detector
\end{enumerate}

\subsection{Model 1: Entity Reaction Profiler}

\subsubsection{Objective}
Predict Trump's emotional reaction to specific entities (people, countries, organizations).

\subsubsection{Methodology}
\begin{enumerate}
    \item Aggregate sentiment scores for each entity across all speeches
    \item Compute baseline sentiment (global average)
    \item Calculate centered sentiment: $s_{centered} = s_{raw} - s_{baseline}$
    \item Classify reactions based on sentiment and emotion ratios
\end{enumerate}

\subsubsection{Reaction Classification}
\begin{itemize}
    \item \textbf{CELEBRATION\_MODE:} High positive sentiment, low negative emotions
    \item \textbf{ATTACK\_MODE:} High negative sentiment, high anger/fear
    \item \textbf{CRITICISM\_MODE:} Moderate negative sentiment
    \item \textbf{NEUTRAL\_MODE:} Near-baseline sentiment
    \item \textbf{UNPREDICTABLE:} High sentiment volatility
\end{itemize}

\subsubsection{Output}
\begin{lstlisting}
{
    'entity': 'China',
    'reaction_type': 'CRITICISM_MODE',
    'sentiment_label': 'NEGATIVE',
    'volatility': 'MEDIUM',
    'confidence': 0.75
}
\end{lstlisting}

\subsection{Model 2: Personality Compatibility Predictor}

\subsubsection{Objective}
Predict how Trump will respond to individuals with different personality types.

\subsubsection{Theoretical Framework}
Based on the \textbf{Big Five Personality Model (OCEAN)}:
\begin{itemize}
    \item \textbf{O}penness to Experience
    \item \textbf{C}onscientiousness
    \item \textbf{E}xtraversion
    \item \textbf{A}greeableness
    \item \textbf{N}euroticism
\end{itemize}

\subsubsection{Trump's Derived Profile}
Extracted from speech patterns:
\begin{table}[H]
    \centering
    \caption{Trump's Big Five Profile (Estimated)}
    \begin{tabular}{lr}
        \toprule
        \textbf{Trait} & \textbf{Score (0-100)} \\
        \midrule
        Openness & 45 \\
        Conscientiousness & 55 \\
        Extraversion & 85 \\
        Agreeableness & 25 \\
        Neuroticism & 60 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Compatibility Scoring}
Using Interpersonal Circumplex Theory:
\begin{equation}
    \text{Compatibility} = f(\text{Dominance Match}, \text{Warmth Match}, \text{Trait Differences})
\end{equation}

\subsubsection{Response Categories}
\begin{itemize}
    \item COOPERATIVE (score > 70)
    \item TRANSACTIONAL (score 50-70)
    \item COMPETITIVE (score 30-50)
    \item HOSTILE (score < 30)
\end{itemize}

\subsection{Model 3: Negotiation Success Predictor}

\subsubsection{Objective}
Predict likelihood of successful negotiation based on topic, communication style, and approach strategies.

\subsubsection{Input Parameters}
\begin{itemize}
    \item \textbf{Topic:} trade, economy, immigration, security, etc.
    \item \textbf{Communication Style:} flattering, transactional, assertive, diplomatic
    \item \textbf{Strategies:} show\_win, media\_angle, business\_frame, loyalty\_appeal
\end{itemize}

\subsubsection{Prediction Formula}
\begin{equation}
    P_{success} = P_{base}(topic) \times E_{style} \times \prod_{i} M_{strategy_i}
\end{equation}

Where:
\begin{itemize}
    \item $P_{base}$: Base probability by topic
    \item $E_{style}$: Style effectiveness multiplier
    \item $M_{strategy}$: Strategy multipliers (boost or penalty)
\end{itemize}

\subsubsection{Topic Favorability}
\begin{table}[H]
    \centering
    \caption{Topic Favorability Scores}
    \begin{tabular}{lr}
        \toprule
        \textbf{Topic} & \textbf{Favorability (\%)} \\
        \midrule
        Military & 90 \\
        Trade & 85 \\
        Economy & 85 \\
        Immigration & 80 \\
        Media & 30 \\
        Environment & 25 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Model 4: Response Classifier (ML)}

\subsubsection{Objective}
Classify Trump's likely response type using machine learning.

\subsubsection{Response Categories}
\begin{itemize}
    \item ATTACK
    \item PRAISE
    \item NEGOTIATE
    \item DEFLECT
    \item NEUTRAL
\end{itemize}

\subsubsection{Features Used}
\begin{itemize}
    \item sentiment\_compound
    \item sentiment\_neg, sentiment\_pos
    \item power\_affiliation\_ratio
    \item certainty markers
    \item topic indicators
\end{itemize}

\subsubsection{Model Architecture}
\begin{itemize}
    \item \textbf{Algorithm:} Random Forest Classifier
    \item \textbf{Alternative:} Gradient Boosting Classifier
    \item \textbf{Feature Scaling:} StandardScaler
    \item \textbf{Train/Test Split:} 75/25 with stratification
\end{itemize}

\subsubsection{Training Process}
\begin{lstlisting}
# Label generation (rule-based)
def classify_response_type(row):
    if row['sentiment_neg'] > 0.15:
        return 'ATTACK'
    elif row['sentiment_pos'] > 0.2:
        return 'PRAISE'
    # ... additional rules

# Model training
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42
)
rf_model.fit(X_train, y_train)
\end{lstlisting}

\subsection{Model 5: Psychological Influence Model}

\subsubsection{Objective}
Recommend effective persuasion tactics based on Trump's psychological profile.

\subsubsection{Theoretical Framework}
Based on \textbf{Cialdini's Six Principles of Persuasion}:
\begin{enumerate}
    \item \textbf{Reciprocity:} Tendency to return favors
    \item \textbf{Commitment/Consistency:} Desire to be consistent
    \item \textbf{Social Proof:} Following others' actions
    \item \textbf{Authority:} Deference to experts
    \item \textbf{Liking:} Persuaded by people we like
    \item \textbf{Scarcity:} Value rare opportunities
\end{enumerate}

\subsubsection{Trump's Psychological Profile}
Derived from speech analysis:
\begin{itemize}
    \item \textbf{Ego Score:} High (based on pronoun usage)
    \item \textbf{Approval Seeking:} High
    \item \textbf{Control Need:} Very High
    \item \textbf{Competitiveness:} Very High
    \item \textbf{Status Consciousness:} Very High
    \item \textbf{Emotional Reactivity:} High
\end{itemize}

\subsubsection{Influence Effectiveness Ranking}
\begin{table}[H]
    \centering
    \caption{Influence Principle Effectiveness}
    \begin{tabular}{lr}
        \toprule
        \textbf{Principle} & \textbf{Effectiveness (\%)} \\
        \midrule
        Liking (Flattery) & 90 \\
        Authority & 75 \\
        Scarcity & 70 \\
        Reciprocity & 65 \\
        Social Proof & 60 \\
        Commitment & 55 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Model 6: Trigger Word Detector}

\subsubsection{Objective}
Identify words and phrases that trigger strong emotional responses.

\subsubsection{Methodology}
\begin{enumerate}
    \item Extract all unique words from corpus
    \item Compute co-occurrence with high-emotion segments
    \item Calculate trigger score based on emotion intensity
    \item Classify by valence (positive/negative)
\end{enumerate}

\subsubsection{Trigger Score Computation}
\begin{equation}
    \text{Trigger Score} = \alpha \cdot \text{Emotion Intensity} + \beta \cdot \text{Frequency} + \gamma \cdot \text{Sentiment Deviation}
\end{equation}

\subsubsection{Example Trigger Words}
\begin{table}[H]
    \centering
    \caption{High Trigger Words}
    \begin{tabular}{lrl}
        \toprule
        \textbf{Word/Phrase} & \textbf{Score} & \textbf{Valence} \\
        \midrule
        ``fake news'' & 95 & Negative \\
        ``winning'' & 90 & Positive \\
        ``disaster'' & 88 & Negative \\
        ``beautiful'' & 85 & Positive \\
        ``corrupt'' & 82 & Negative \\
        \bottomrule
    \end{tabular}
\end{table}

% ============================================================================
% 9. UNIFIED MODEL
% ============================================================================
\section{Mega Trump Model: Unified Predictor}

\subsection{Architecture}
The Mega Trump Model combines all six sub-models into a unified prediction system.

\begin{figure}[H]
    \centering
    \begin{verbatim}
    +------------------+
    |   User Inputs    |
    +------------------+
            |
            v
    +------------------+
    | MegaTrumpPredictor|
    +------------------+
            |
    +-------+-------+-------+-------+-------+-------+
    |       |       |       |       |       |       |
    v       v       v       v       v       v       v
  Entity  Person  Negot.  Response Influence Trigger
  React.  Compat. Success Classif.  Model   Detect.
    |       |       |       |       |       |       |
    +-------+-------+-------+-------+-------+-------+
            |
            v
    +------------------+
    | Weighted Ensemble|
    +------------------+
            |
            v
    +------------------+
    | Unified Output   |
    +------------------+
    \end{verbatim}
    \caption{Mega Trump Model Architecture}
\end{figure}

\subsection{Model Weights}
\begin{table}[H]
    \centering
    \caption{Sub-Model Weights}
    \begin{tabular}{lr}
        \toprule
        \textbf{Sub-Model} & \textbf{Weight} \\
        \midrule
        Entity Reaction & 0.25 \\
        Personality Compatibility & 0.20 \\
        Negotiation Success & 0.20 \\
        Response Classifier & 0.15 \\
        Influence Strategy & 0.10 \\
        Trigger Word Detector & 0.10 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Behavioral Categories}
Predictions are mapped to five behavioral categories:
\begin{enumerate}
    \item \textbf{COOPERATIVE:} Positive engagement likely
    \item \textbf{TRANSACTIONAL:} Business-focused interaction
    \item \textbf{COMPETITIVE:} Power dynamics expected
    \item \textbf{HOSTILE:} High conflict risk
    \item \textbf{NEUTRAL:} Uncertain/mixed signals
\end{enumerate}

\subsection{Confidence Calculation}
\begin{equation}
    \text{Confidence} = \text{Winning Category Score} \times \min(95, 50 + n_{models} \times 7.5)
\end{equation}

Where $n_{models}$ is the number of active sub-models (1-6).

\subsection{Output Structure}
\begin{lstlisting}
{
    'overall_prediction': 'COOPERATIVE',
    'confidence': 75.5,
    'category_scores': {
        'COOPERATIVE': 75.5,
        'TRANSACTIONAL': 15.2,
        'COMPETITIVE': 5.0,
        'HOSTILE': 2.3,
        'NEUTRAL': 2.0
    },
    'sub_model_contributions': {...},
    'recommendations': [
        "Trump is likely to be receptive",
        "Maintain positive framing",
        "Best influence tactic: Liking"
    ]
}
\end{lstlisting}

% ============================================================================
% 10. INTERACTIVE DASHBOARD
% ============================================================================
\section{Interactive Dashboard}

\subsection{Technology Stack}
\begin{itemize}
    \item \textbf{Framework:} Streamlit
    \item \textbf{Language:} Python 3.x
    \item \textbf{Visualization:} Native Streamlit components
\end{itemize}

\subsection{Features}
\begin{enumerate}
    \item Model selection sidebar (7 models)
    \item Dynamic input forms for each model
    \item Real-time predictions
    \item Visual confidence indicators
    \item Recommendation display
    \item Sub-model contribution breakdown
\end{enumerate}

\subsection{Usage}
\begin{lstlisting}[language=bash]
# Run the dashboard
streamlit run app.py
\end{lstlisting}

% ============================================================================
% 11. CONCLUSION
% ============================================================================
\section{Conclusion}

\subsection{Summary of Achievements}
This project successfully implemented:

\begin{enumerate}
    \item A complete web scraping pipeline for transcript collection
    \item Comprehensive data cleaning and preprocessing
    \item Advanced NLP transformation with sentiment and emotion analysis
    \item Feature engineering with 80+ derived attributes
    \item Star schema data warehouse design
    \item Six specialized predictive models
    \item Unified Mega Trump Model for comprehensive predictions
    \item Interactive Streamlit dashboard
\end{enumerate}

\subsection{Key Findings}

\subsubsection{Linguistic Patterns}
\begin{itemize}
    \item Average reading level: 5th-6th grade (highly accessible)
    \item High repetition and anaphora usage (persuasive style)
    \item Frequent superlatives (hyperbolic language)
\end{itemize}

\subsubsection{Emotional Profile}
\begin{itemize}
    \item Overwhelmingly positive sentiment at speech level (97\%+)
    \item High trust and anticipation emotions
    \item Controlled anger and fear for rhetorical effect
\end{itemize}

\subsubsection{Psychological Indicators}
\begin{itemize}
    \item Ego-focused communication (high I/We ratio)
    \item Power-oriented language
    \item High certainty markers indicating confidence projection
\end{itemize}

\subsection{Limitations}
\begin{itemize}
    \item Data limited to Rev.com transcripts
    \item Emotion classification based on lexicon (not ML)
    \item Predictive models based on observed patterns, not causal relationships
    \item Response classifier trained on rule-generated labels
\end{itemize}

\subsection{Future Work}
\begin{enumerate}
    \item Expand dataset with historical speeches
    \item Implement deep learning for emotion classification
    \item Add real-time transcript analysis capability
    \item Develop comparative analysis with other political figures
    \item Enhance ML models with larger training data
\end{enumerate}

% ============================================================================
% REFERENCES
% ============================================================================
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{enumerate}
    \item Hutto, C.J. \& Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. \textit{ICWSM}.
    
    \item Mohammad, S.M. \& Turney, P.D. (2013). Crowdsourcing a Word-Emotion Association Lexicon. \textit{Computational Intelligence}.
    
    \item Cialdini, R.B. (2009). \textit{Influence: Science and Practice}. Pearson.
    
    \item Pennebaker, J.W. et al. (2015). The Development and Psychometric Properties of LIWC2015. University of Texas at Austin.
    
    \item spaCy: Industrial-Strength Natural Language Processing. https://spacy.io/
    
    \item Streamlit: The fastest way to build data apps. https://streamlit.io/
\end{enumerate}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix
\section{Project Structure}
\begin{verbatim}
donald_trump/
+-- scripts/
|   +-- scrape.py              # Web scraping
|   +-- 01_data_cleaning.py    # Data cleaning
|   +-- 02_data_transformation.py  # NLP pipeline
|   +-- 03_feature_engineering.py  # Feature extraction
|   +-- 04_analysis_suite.py   # Descriptive analytics
|   +-- 05_entity_extraction.py    # NER cataloging
|   +-- 06_entity_relationships.py # Entity mapping
|   +-- 07_etl_extraction.py   # ETL extraction
|   +-- 08_etl_preprocessing.py    # ETL preprocessing
|   +-- 09_etl_load_warehouse.py   # ETL loading
|   +-- 10_data_quality.py     # Quality validation
+-- notebooks/
|   +-- 01-06: Exploratory analysis
|   +-- 07_predictive_entity_reaction.ipynb
|   +-- 08_trigger_word_detector.ipynb
|   +-- 09_personality_compatibility.ipynb
|   +-- 10_negotiation_predictor.ipynb
|   +-- 11_response_classifier.ipynb
|   +-- 12_influence_model.ipynb
+-- data/
|   +-- raw/           # Scraped data
|   +-- cleaned/       # Cleaned data
|   +-- transformed/   # NLP features
|   +-- entities/      # Entity catalogs
|   +-- staging/       # ETL staging
|   +-- warehouse/     # Data warehouse
|   +-- results/       # Analysis results
+-- sql/
|   +-- 01_create_schema.sql   # Star schema DDL
|   +-- 02_create_indexes.sql  # Index definitions
|   +-- 04_load_data.sql       # Generated inserts
+-- app.py             # Streamlit dashboard
+-- mega_trump_model.py    # Unified predictor
+-- config.yaml        # Configuration
+-- requirements.txt   # Dependencies
\end{verbatim}

\section{Configuration File}
Key configuration parameters in \texttt{config.yaml}:
\begin{itemize}
    \item File paths for all data directories
    \item Cleaning parameters (reaction tags, noise tokens)
    \item NLP settings (spaCy model, embedding model)
    \item Feature engineering parameters (anaphora patterns, keyword clusters)
    \item Analysis settings (LDA topics, correlation thresholds)
\end{itemize}

\end{document}

